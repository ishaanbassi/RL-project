{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import division\n",
    "import gym\n",
    "import torch\n",
    "import random\n",
    "import numpy as np\n",
    "import torch\n",
    "from PIL import Image\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from collections import namedtuple\n",
    "from collections import deque\n",
    "import torch.nn.functional as F\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "from matplotlib.pyplot import imshow\n",
    "from PIL import Image\n",
    "from wrappers import make_atari, wrap_deepmind, wrap_pytorch\n",
    "import queue\n",
    "from torch import optim\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "import pandas as pd \n",
    "import os \n",
    "import pickle "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class QNet(torch.nn.Module):\n",
    "    def __init__(self,obs_shape,act_shape):\n",
    "        super(QNet, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(4, 32, kernel_size=8, stride=4)\n",
    "        self.conv2 = nn.Conv2d(32, 64, kernel_size=4, stride=2)\n",
    "        self.conv3 = nn.Conv2d(64, 64, kernel_size=3, stride=1)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.fc1 = nn.Linear(7*7*64,512)\n",
    "        self.fc2 = nn.Linear(512,1)\n",
    "        self.fc3 = nn.Linear(7*7*64,512)\n",
    "        self.fc4 = nn.Linear(512,act_shape)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        #Conv\n",
    "        x = self.conv1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.conv2(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.conv3(x)\n",
    "        x = self.relu(x)\n",
    "        x = x.view(x.shape[0],-1)\n",
    "        \n",
    "        #Fc\n",
    "        x1 = x\n",
    "        x = self.fc1(x)\n",
    "        x = self.relu(x)\n",
    "        v = self.fc2(x)\n",
    "        \n",
    "        x1 = self.fc3(x1)\n",
    "        x1 = self.relu(x1)\n",
    "        adv = self.fc4(x1)\n",
    "        \n",
    "        q_s_a = v + adv - adv.mean()\n",
    "        \n",
    "        return q_s_a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eps_greedy(epsilon,state,net):\n",
    "    if(np.random.random()<epsilon):\n",
    "        action = np.random.randint(ACT_SHAPE)\n",
    "    else:\n",
    "        qvalues = net(state)\n",
    "        action = torch.argmax(qvalues).item()\n",
    "    return action    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "env    = make_atari('PongNoFrameskip-v4')\n",
    "env    = wrap_deepmind(env)\n",
    "env    = wrap_pytorch(env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(net,evaluation_episodes):\n",
    "    state = env.reset()\n",
    "    net.eval()\n",
    "    state = torch.Tensor(state).cuda()\n",
    "    state = state.unsqueeze(0)\n",
    "    rewards = []\n",
    "    count = 0\n",
    "    episode_reward = 0\n",
    "    while(True):\n",
    "        action = eps_greedy(0,state,net)  \n",
    "        next_state, reward, done,info = env.step(action)\n",
    "        next_state = torch.Tensor(next_state).cuda().unsqueeze(0)\n",
    "        state = next_state\n",
    "        episode_reward += reward\n",
    "        if done:\n",
    "            count += 1\n",
    "            print('Episode ',count,end=' ')\n",
    "            print('Reward ',episode_reward)\n",
    "            rewards.append(episode_reward)\n",
    "            state = env.reset()\n",
    "            state = torch.Tensor(state).cuda()\n",
    "            state = state.unsqueeze(0)\n",
    "            episode_reward = 0\n",
    "        if(count == evaluation_episodes):\n",
    "            break\n",
    "    env.close()\n",
    "\n",
    "\n",
    "    return sum(rewards)/len(rewards)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode  1 Reward  21.0\n",
      "Episode  2 Reward  20.0\n",
      "Episode  3 Reward  21.0\n",
      "Episode  4 Reward  20.0\n",
      "Episode  5 Reward  21.0\n",
      "Episode  6 Reward  21.0\n",
      "Episode  7 Reward  21.0\n",
      "Episode  8 Reward  21.0\n",
      "Episode  9 Reward  19.0\n",
      "Episode  10 Reward  21.0\n",
      "Episode  11 Reward  21.0\n",
      "Episode  12 Reward  20.0\n",
      "Episode  13 Reward  20.0\n",
      "Episode  14 Reward  21.0\n",
      "Episode  15 Reward  21.0\n",
      "Episode  16 Reward  21.0\n",
      "Episode  17 Reward  20.0\n",
      "Episode  18 Reward  21.0\n",
      "Episode  19 Reward  21.0\n",
      "Episode  20 Reward  21.0\n",
      "Episode  21 Reward  21.0\n",
      "Episode  22 Reward  21.0\n",
      "Episode  23 Reward  19.0\n",
      "Episode  24 Reward  21.0\n",
      "Episode  25 Reward  20.0\n",
      "Episode  26 Reward  21.0\n",
      "Episode  27 Reward  21.0\n",
      "Episode  28 Reward  21.0\n",
      "Episode  29 Reward  19.0\n",
      "Episode  30 Reward  19.0\n",
      "Episode  31 Reward  21.0\n",
      "Episode  32 Reward  21.0\n",
      "Episode  33 Reward  20.0\n",
      "Episode  34 Reward  20.0\n",
      "Episode  35 Reward  21.0\n",
      "Episode  36 Reward  19.0\n",
      "Episode  37 Reward  20.0\n",
      "Episode  38 Reward  20.0\n",
      "Episode  39 Reward  19.0\n",
      "Episode  40 Reward  19.0\n",
      "Episode  41 Reward  20.0\n",
      "Episode  42 Reward  21.0\n",
      "Episode  43 Reward  20.0\n",
      "Episode  44 Reward  21.0\n",
      "Episode  45 Reward  21.0\n",
      "Episode  46 Reward  20.0\n",
      "Episode  47 Reward  19.0\n",
      "Episode  48 Reward  21.0\n",
      "Episode  49 Reward  20.0\n",
      "Episode  50 Reward  21.0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "20.4"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net = QNet(env.observation_space.shape,env.action_space.n).cuda()\n",
    "net.load_state_dict(torch.load('./dueling-logs/dueling-model2000000.pth'))\n",
    "test(net,50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
