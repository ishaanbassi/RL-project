{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import division\n",
    "import gym\n",
    "import torch\n",
    "import random\n",
    "import numpy as np\n",
    "import torch\n",
    "from PIL import Image\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from collections import namedtuple\n",
    "from collections import deque\n",
    "import torch.nn.functional as F\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "from matplotlib.pyplot import imshow\n",
    "from PIL import Image\n",
    "from wrappers import make_atari, wrap_deepmind, wrap_pytorch\n",
    "import queue\n",
    "from torch import optim\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "import pandas as pd \n",
    "import os \n",
    "import pickle "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class QNet(torch.nn.Module):\n",
    "    def __init__(self,obs_shape,act_shape):\n",
    "        super(QNet, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(4, 32, kernel_size=8, stride=4)\n",
    "        self.conv2 = nn.Conv2d(32, 64, kernel_size=4, stride=2)\n",
    "        self.conv3 = nn.Conv2d(64, 64, kernel_size=3, stride=1)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.fc1 = nn.Linear(7*7*64,512)\n",
    "        self.fc2 = nn.Linear(512,act_shape)\n",
    "#         self.fc3 = nn.Linear(7*7*64,512)\n",
    "#         self.fc4 = nn.Linear(512,act_shape)\n",
    "    def forward(self, x):\n",
    "        #Conv\n",
    "        x = self.conv1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.conv2(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.conv3(x)\n",
    "        x = self.relu(x)\n",
    "        x = x.view(x.shape[0],-1)\n",
    "        \n",
    "        #Fc\n",
    "        x1 = x\n",
    "        x = self.fc1(x)\n",
    "        x = self.relu(x)\n",
    "        q_s_a = self.fc2(x)\n",
    "        \n",
    "#         x1 = self.fc3(x1)\n",
    "#         x1 = self.relu(x1)\n",
    "#         adv = self.fc4(x1)\n",
    "        \n",
    "#         q_s_a = v + adv - adv.mean()\n",
    "        \n",
    "        return q_s_a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eps_greedy(epsilon,state,net):\n",
    "    if(np.random.random()<epsilon):\n",
    "        action = np.random.randint(ACT_SHAPE)\n",
    "    else:\n",
    "        qvalues = net(state)\n",
    "        action = torch.argmax(qvalues).item()\n",
    "    return action    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "env    = make_atari('PongNoFrameskip-v4')\n",
    "env    = wrap_deepmind(env)\n",
    "env    = wrap_pytorch(env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(net,evaluation_episodes):\n",
    "    state = env.reset()\n",
    "    net.eval()\n",
    "    state = torch.Tensor(state).cuda()\n",
    "    state = state.unsqueeze(0)\n",
    "    rewards = []\n",
    "    count = 0\n",
    "    episode_reward = 0\n",
    "    while(True):\n",
    "        action = eps_greedy(0,state,net)  \n",
    "        next_state, reward, done,info = env.step(action)\n",
    "        next_state = torch.Tensor(next_state).unsqueeze(0).cuda()\n",
    "        state = next_state\n",
    "        episode_reward += reward\n",
    "        if done:\n",
    "            count += 1\n",
    "            print('Episode ',count,end=' ')\n",
    "            print('Reward ',episode_reward)\n",
    "            rewards.append(episode_reward)\n",
    "            state = env.reset()\n",
    "            state = torch.Tensor(state).cuda()\n",
    "            state = state.unsqueeze(0)\n",
    "            episode_reward = 0\n",
    "        if(count == evaluation_episodes):\n",
    "            break\n",
    "    env.close()\n",
    "\n",
    "\n",
    "    return sum(rewards)/len(rewards)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode  1 Reward  21.0\n"
     ]
    }
   ],
   "source": [
    "net = QNet(env.observation_space.shape,env.action_space.n).cuda()\n",
    "net.load_state_dict(torch.load('./prioritized-logs/prioritized-model1000000.pth'))\n",
    "test(net,50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
