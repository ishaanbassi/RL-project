{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import division\n",
    "import gym\n",
    "import torch\n",
    "import random\n",
    "import numpy as np\n",
    "import torch\n",
    "from PIL import Image\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from collections import namedtuple\n",
    "from collections import deque\n",
    "import torch.nn.functional as F\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "from matplotlib.pyplot import imshow\n",
    "from PIL import Image\n",
    "from wrappers import make_atari, wrap_deepmind, wrap_pytorch\n",
    "import queue\n",
    "from torch import optim\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class QNet(torch.nn.Module):\n",
    "    def __init__(self,obs_shape,act_shape):\n",
    "        super(QNet, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(4, 32, kernel_size=8, stride=4)\n",
    "        self.conv2 = nn.Conv2d(32, 64, kernel_size=4, stride=2)\n",
    "        self.conv3 = nn.Conv2d(64, 64, kernel_size=3, stride=1)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.fc1 = nn.Linear(7*7*64,512)\n",
    "        self.fc2 = nn.Linear(512,act_shape)\n",
    "#         self.fc3 = nn.Linear(7*7*64,512)\n",
    "#         self.fc4 = nn.Linear(512,act_shape)\n",
    "    def forward(self, x):\n",
    "        #Conv\n",
    "        x = self.conv1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.conv2(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.conv3(x)\n",
    "        x = self.relu(x)\n",
    "        x = x.view(x.shape[0],-1)\n",
    "        \n",
    "        #Fc\n",
    "        x1 = x\n",
    "        x = self.fc1(x)\n",
    "        x = self.relu(x)\n",
    "        q_s_a = self.fc2(x)\n",
    "        \n",
    "#         x1 = self.fc3(x1)\n",
    "#         x1 = self.relu(x1)\n",
    "#         adv = self.fc4(x1)\n",
    "        \n",
    "#         q_s_a = v + adv - adv.mean()\n",
    "        \n",
    "        return q_s_a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eps_greedy(epsilon,state,net):\n",
    "    if(np.random.random()<epsilon):\n",
    "        action = np.random.randint(ACT_SHAPE)\n",
    "    else:\n",
    "        qvalues = net(state)\n",
    "        action = torch.argmax(qvalues).item()\n",
    "    return action    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReplayBuffer(object):\n",
    "    def __init__(self,maxsize):\n",
    "        self.q = deque(maxlen = maxsize)\n",
    "        self.maxsize = maxsize\n",
    "    def add(self,x):\n",
    "        self.q.append(x)\n",
    "        if(len(self.q)==self.maxsize):\n",
    "            self.q.popleft()\n",
    "    def getSize(self):\n",
    "        return len(self.q)\n",
    "    def sample(self,size):\n",
    "        batch = random.sample(list(self.q),size)\n",
    "        state,action,reward,next_state,done = map(list, zip(*batch))\n",
    "        return state,action,reward,next_state,done\n",
    "           "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SegmentSumTree:\n",
    "    def __init__(self, capacity):\n",
    "        self.size = capacity \n",
    "        #n leaves + n-1 internal = 2n-1, capacity of a tree\n",
    "        self.tree = np.zeros((2*capacity-1), dtype = np.float32)\n",
    "        #leaf nodes having actual values \n",
    "        self.transit_data_buffer =  np.array([None] * self.size) \n",
    "        self.is_full = False\n",
    "        self.max_prior_val = 1\n",
    "        self.num_entries = 0 \n",
    "        self.idx = 0 \n",
    "    \n",
    "    \n",
    "    def append(self, priority_val, transit_data):\n",
    "        #self.max_prior_val = max(priority_val, self.max_prior_val)\n",
    "        self.transit_data_buffer[self.idx] = transit_data  \n",
    "        self.update(self.idx + self.size - 1, priority_val) \n",
    "        self.idx = (self.idx + 1) % self.size \n",
    "        self.num_entries += 1\n",
    "        if self.num_entries > self.size:\n",
    "            self.num_entries = self.size\n",
    "        self.is_full = self.is_full or self.idx == 0  \n",
    "    \n",
    "    \n",
    "    def propagate(self, index, priority_val):\n",
    "        parent_idx = self.getParentIdx(index)\n",
    "        left_node_idx = 2 * parent_idx + 1\n",
    "        right_node_idx = 2 * parent_idx + 2\n",
    "        self.tree[parent_idx] = self.tree[left_node_idx] + self.tree[right_node_idx]\n",
    "        if parent_idx != 0: \n",
    "            self.propagate(parent_idx, priority_val)\n",
    "            \n",
    "        \n",
    "    def update(self, index, priority_val):\n",
    "#         print('priority',priority_val)\n",
    "        self.tree[index] = priority_val\n",
    "        self.max_prior_val = max(priority_val, self.max_prior_val)\n",
    "        self.propagate(index, priority_val)\n",
    "       \n",
    "    \n",
    "    def getParentIdx(self, index):\n",
    "        return (index - 1)//2\n",
    "        \n",
    "            \n",
    "        \n",
    "    def search(self, value):\n",
    "        idx = self.retrieve(0, value)  # Search for index of item from root\n",
    "        data_index = idx - self.size + 1\n",
    "        return (self.tree[idx], data_index, idx)\n",
    "        \n",
    "    \n",
    "    def retrieve(self, idx, value):\n",
    "        left_node_idx = 2 * idx + 1\n",
    "        right_node_idx = 2 * idx + 2\n",
    "        \n",
    "        if left_node_idx >= len(self.tree):\n",
    "            return idx\n",
    "        \n",
    "        elif value <= self.tree[left_node_idx]:\n",
    "            return self.retrieve(left_node_idx, value)\n",
    "            \n",
    "        else:\n",
    "            return self.retrieve(right_node_idx, value - self.tree[left_node_idx])\n",
    "        \n",
    "    \n",
    "    def getNumEntries(self):\n",
    "        return self.num_entries\n",
    "     \n",
    "\n",
    "    def getTotal(self):\n",
    "        return self.tree[0]\n",
    "        \n",
    "    def getSize(self):\n",
    "        return self.size\n",
    "    \n",
    "    \n",
    "    def getMaxPriorValue(self):\n",
    "        return self.max_prior_val\n",
    "    \n",
    "    \n",
    "    def getPriorties(self):\n",
    "        return self.priorities\n",
    "    \n",
    "    \n",
    "    def getTree(self):\n",
    "        return self.tree\n",
    "    \n",
    "    \n",
    "    def getDataByIdx(self, idx):\n",
    "        return self.transit_data_buffer[idx % self.size]\n",
    "    \n",
    "    \n",
    "Transition = namedtuple('Transition', ('state', 'action', 'reward', 'next_state','done'))\n",
    "\n",
    "class PrioritizedReplayBuffer:\n",
    "    def __init__(self,capacity = 1000000, batch_size = 32, gamma = 0.99, multi_step = 3, \n",
    "                beta = 0.4, alpha = 0.5):\n",
    "        self.capacity = capacity\n",
    "        self.discount = gamma\n",
    "        self.n = multi_step\n",
    "        self.beta = beta   \n",
    "        self.alpha = alpha\n",
    "        self.transit_buffer = SegmentSumTree(capacity) \n",
    "               \n",
    "\n",
    "    def add(self,state,action,reward,next_state,done):\n",
    "        \n",
    "        self.transit_buffer.append(self.transit_buffer.max_prior_val, Transition(state, action, reward, next_state, done))  \n",
    "        \n",
    "        \n",
    "    def sample(self,k): \n",
    "        batch, idxs, priorities = [], [], []\n",
    "        root_total_priority = self.transit_buffer.getTotal()\n",
    "        segment = root_total_priority / k\n",
    "        \n",
    "#         priority_exponent_weight = (1 - self.beta) / (TMAX - TMIN)\n",
    "        priority_exponent_weight = (1 - self.beta) / 1000\n",
    "        self.beta = np.min([1.0, self.beta + priority_exponent_weight])\n",
    "#         print('total priority',root_total_priority)\n",
    "        \n",
    "        for i in range(k):\n",
    "            a = segment * i\n",
    "            b = segment * (i + 1)\n",
    "            samp = random.uniform(a, b)\n",
    "#             print('sample : ',samp)\n",
    "            priority, data_idx, idx = self.transit_buffer.search(samp)\n",
    "#             print('data idx',data_idx)\n",
    "            data = self.transit_buffer.getDataByIdx(data_idx)\n",
    "            priorities.append(priority)\n",
    "            batch.append(data)\n",
    "            idxs.append(idx)\n",
    "        #print('*'*100)\n",
    "        sum_priority = np.power(self.transit_buffer.tree[:self.transit_buffer.getNumEntries()],self.alpha)\n",
    "        sum_priority = sum_priority.sum()\n",
    "        sampling_probabilities = np.power(priorities, self.alpha) / sum_priority #root_total_priority\n",
    "        #\n",
    "        \n",
    "        #print(sampling_probabilities)\n",
    "        \n",
    "        #compute importance sampling \n",
    "        weights = np.power(self.transit_buffer.getNumEntries() * sampling_probabilities, -self.beta)\n",
    "        weights /= weights.max()\n",
    "        \n",
    "#         try:\n",
    "        states, actions, rewards, next_states, dones = zip(*batch)\n",
    "#         except:\n",
    "#             print('error in ',batch)\n",
    "        \n",
    "        states,actions, rewards, next_states, dones = list(states), list(actions), list(rewards), \\\n",
    "                                                    list(next_states), list(dones) \n",
    "        states = torch.cat([x for x in states]).cuda()\n",
    "        next_states = torch.cat([x for x in next_states]).cuda()\n",
    "        actions = torch.Tensor(actions).long().cuda()\n",
    "        dones = np.array(dones).astype(int)\n",
    "        dones = torch.Tensor(dones).cuda()\n",
    "        rewards = torch.Tensor(rewards).cuda()\n",
    "        #priorities = torch.Tensor(priorities).cuda()\n",
    "        weights = torch.Tensor(weights).cuda()\n",
    "        \n",
    "        return weights, idxs, states, actions, rewards, next_states, dones       \n",
    "        \n",
    "        \n",
    "    def updatePriorities(self, indexes, priorities):\n",
    "        #priorities = np.power(priorities, self.alpha)\n",
    "        for idx, priority_val in zip(indexes, priorities):\n",
    "            self.transit_buffer.update(idx, priority_val)\n",
    "        \n",
    "        \n",
    "    def getSize(self):\n",
    "        return self.transit_buffer.getNumEntries()\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loss function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_loss(size):\n",
    "    weights,idx,current_state,action,reward,next_state,done = buffer.sample(size)\n",
    "    \n",
    "    \n",
    "    qvalues = net(current_state)\n",
    "    qvalues_next = net(next_state)\n",
    "    target_net.eval()\n",
    "    qvalues_target = target_net(next_state)\n",
    "    \n",
    "    #Q(s,a)\n",
    "    q_a = qvalues.gather(1, action.unsqueeze(1)).squeeze(1)\n",
    "    \n",
    "    #Selecting action for target network\n",
    "    selected = qvalues_next.max(1)[1]\n",
    "    #Q'(s',argmax Q(s',a))\n",
    "    q_a_target = qvalues_target.gather(1,selected.unsqueeze(1)).squeeze(1)\n",
    "    \n",
    "    #Computing target value\n",
    "    target = reward + GAMMA * q_a_target * (1 - done)\n",
    "    L = (target - q_a)#.pow(2)*weights\n",
    "    \n",
    "#     print('priority : ',weights)\n",
    "    new_priorities =  torch.abs(L) + edge_epsilon\n",
    "#     L = torch.clamp(L,-1,1)\n",
    "    L = L.pow(2) * weights \n",
    "    L = L.mean()\n",
    "    optimizer.zero_grad()\n",
    "    L.backward()\n",
    "    optimizer.step()\n",
    "    new_priorities = new_priorities.cpu().detach().numpy()\n",
    "    buffer.updatePriorities(idx,new_priorities)\n",
    "#     print(\"Loss : \",L)\n",
    "    \n",
    "    \n",
    "    return L"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_target():\n",
    "    target_net.load_state_dict(net.state_dict())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def epsilon_decay(ep):\n",
    "    e = .01 + .99*np.exp(-ep/30000)\n",
    "    return e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def addreward(id,item,filename):\n",
    "    f=open(filename,'a+')\n",
    "    f.write(str(id)+' '+str(item)+' '+'\\n')\n",
    "    f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def addloss(id,loss,filename):\n",
    "    f=open(filename,'a+')\n",
    "    f.write(str(id)+' '+str(loss.item())+' '+'\\n')\n",
    "    f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Box(4, 84, 84)\n",
      "Discrete(6)\n"
     ]
    }
   ],
   "source": [
    "# env = gym.make('PongNoFrameskip-v4')\n",
    "env    = make_atari('PongNoFrameskip-v4')\n",
    "# env    = make_atari('BankHeist-v0')\n",
    "# env    = make_atari('Pong-v0')\n",
    "env    = wrap_deepmind(env)\n",
    "env    = wrap_pytorch(env)\n",
    "\n",
    "print(env.observation_space)\n",
    "print(env.action_space)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 123\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(np.random.randint(1, 10000)) \n",
    "net = QNet(env.observation_space.shape,env.action_space.n)\n",
    "net = net.cuda()\n",
    "target_net = QNet(env.observation_space.shape,env.action_space.n)\n",
    "target_net = target_net.cuda()\n",
    "update_target()\n",
    "optimizer = optim.Adam(net.parameters(), lr=0.0000625,eps=1.5e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "lossfile = './prioritized-logs/losses.csv'\n",
    "rewardsfile = './prioritized-logs/rewards.csv'\n",
    "if os.path.exists(lossfile):\n",
    "    os.remove(lossfile)\n",
    "if os.path.exists(rewardsfile):\n",
    "    os.remove(rewardsfile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "ITERATIONS = 5000000\n",
    "epsilon = 0.99\n",
    "OBS_SHAPE = env.observation_space.shape\n",
    "ACT_SHAPE = env.action_space.n\n",
    "REPLAY_SAMPLE = 5000\n",
    "BATCH_SIZE = 32\n",
    "GAMMA = 0.99\n",
    "T_upd = 10000\n",
    "TMAX = 50e6\n",
    "TMIN = 20e3\n",
    "edge_epsilon = 1e-5\n",
    "save_model_interval = 1e6\n",
    "loss_log_interval = 1e3\n",
    "episode_reward_interval = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/iiitd/myenv/lib/python3.6/site-packages/ipykernel_launcher.py:13: TqdmDeprecationWarning: This function will be removed in tqdm==5.0.0\n",
      "Please use `tqdm.notebook.tqdm` instead of `tqdm.tqdm_notebook`\n",
      "  del sys.path[0]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d7eb141c5da84f4db886c5f7ddb22afd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=5000000), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9297 10 -19.0\n",
      "17368 20 -21.0\n",
      "26123 30 -21.0\n",
      "35153 40 -21.0\n",
      "44389 50 -20.0\n",
      "53298 60 -21.0\n",
      "62511 70 -21.0\n",
      "71976 80 -21.0\n",
      "81689 90 -19.0\n",
      "91341 100 -20.0\n",
      "101574 110 -20.0\n",
      "113484 120 -18.0\n",
      "125522 130 -19.0\n",
      "138940 140 -19.0\n",
      "155448 150 -19.0\n",
      "174731 160 -17.0\n",
      "197762 170 -11.0\n",
      "216124 180 -19.0\n",
      "238945 190 -18.0\n",
      "262065 200 -13.0\n",
      "284843 210 -19.0\n",
      "308257 220 -14.0\n",
      "331174 230 -16.0\n",
      "355987 240 -5.0\n",
      "382295 250 -17.0\n",
      "412917 260 -7.0\n",
      "446906 270 -1.0\n",
      "481180 280 5.0\n",
      "512639 290 7.0\n",
      "540378 300 15.0\n",
      "568167 310 -4.0\n",
      "595745 320 7.0\n",
      "617795 330 18.0\n",
      "637724 340 19.0\n",
      "657911 350 16.0\n",
      "678403 360 20.0\n",
      "697247 370 20.0\n",
      "715706 380 21.0\n",
      "733878 390 20.0\n",
      "752615 400 16.0\n",
      "772372 410 19.0\n",
      "793015 420 19.0\n",
      "812748 430 20.0\n",
      "832315 440 16.0\n",
      "850201 450 21.0\n",
      "867835 460 20.0\n",
      "886014 470 21.0\n",
      "903609 480 21.0\n",
      "921856 490 21.0\n",
      "940301 500 20.0\n",
      "957809 510 20.0\n",
      "975777 520 17.0\n",
      "993586 530 20.0\n",
      "1011634 540 21.0\n",
      "1030358 550 19.0\n",
      "1047894 560 19.0\n",
      "1066465 570 18.0\n",
      "1084372 580 20.0\n",
      "1102086 590 18.0\n",
      "1119800 600 20.0\n",
      "1137962 610 19.0\n",
      "1155960 620 13.0\n",
      "1174553 630 19.0\n",
      "1195237 640 19.0\n",
      "1213556 650 17.0\n",
      "1231789 660 19.0\n",
      "1249625 670 20.0\n",
      "1267877 680 21.0\n",
      "1286671 690 19.0\n",
      "1305923 700 19.0\n",
      "1323440 710 19.0\n",
      "1340764 720 19.0\n",
      "1358470 730 21.0\n",
      "1376458 740 20.0\n",
      "1395497 750 19.0\n",
      "1414452 760 17.0\n",
      "1433886 770 3.0\n",
      "1451715 780 19.0\n",
      "1470677 790 19.0\n",
      "1489387 800 19.0\n",
      "1507666 810 21.0\n",
      "1525084 820 21.0\n",
      "1542424 830 21.0\n",
      "1559749 840 21.0\n",
      "1578260 850 19.0\n",
      "1597302 860 18.0\n",
      "1615139 870 21.0\n",
      "1634219 880 18.0\n",
      "1651622 890 19.0\n",
      "1669863 900 18.0\n",
      "1687815 910 21.0\n",
      "1705647 920 20.0\n",
      "1723985 930 21.0\n",
      "1744004 940 13.0\n",
      "1763406 950 17.0\n",
      "1781638 960 20.0\n",
      "1799151 970 21.0\n",
      "1816835 980 21.0\n",
      "1833611 990 21.0\n",
      "1852285 1000 19.0\n",
      "1870842 1010 16.0\n",
      "1889026 1020 19.0\n",
      "1907709 1030 17.0\n",
      "1925683 1040 16.0\n",
      "1943202 1050 19.0\n",
      "1961351 1060 16.0\n",
      "1979254 1070 19.0\n",
      "1997177 1080 21.0\n",
      "2014659 1090 20.0\n",
      "2032776 1100 21.0\n",
      "2049566 1110 21.0\n",
      "2067165 1120 20.0\n",
      "2084841 1130 20.0\n",
      "2103142 1140 19.0\n",
      "2121930 1150 21.0\n",
      "2139386 1160 20.0\n",
      "2156939 1170 21.0\n",
      "2174147 1180 20.0\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-15-3712da84e521>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     23\u001b[0m     \u001b[0mstate\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnext_state\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m     \u001b[0;32mif\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbuffer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetSize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m>\u001b[0m\u001b[0mREPLAY_SAMPLE\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m%\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m==\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 25\u001b[0;31m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompute_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mBATCH_SIZE\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     26\u001b[0m \u001b[0;31m#         losses.append(loss)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[0;31m#         if(i%100==0):\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-6-716708b5c2ac>\u001b[0m in \u001b[0;36mcompute_loss\u001b[0;34m(size)\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m     \u001b[0mqvalues\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnet\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcurrent_state\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m     \u001b[0mqvalues_next\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnet\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnext_state\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0mtarget_net\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meval\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/myenv/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    539\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    540\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 541\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    542\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    543\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-2-b15bce7af68c>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     22\u001b[0m         \u001b[0;31m#Fc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m         \u001b[0mx1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 24\u001b[0;31m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfc1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     25\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m         \u001b[0mq_s_a\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfc2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/myenv/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    539\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    540\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 541\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    542\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    543\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/myenv/lib/python3.6/site-packages/torch/nn/modules/linear.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m     85\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     86\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 87\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     88\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     89\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mextra_repr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/myenv/lib/python3.6/site-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36mlinear\u001b[0;34m(input, weight, bias)\u001b[0m\n\u001b[1;32m   1368\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m2\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mbias\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1369\u001b[0m         \u001b[0;31m# fused op is marginally faster\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1370\u001b[0;31m         \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maddmm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1371\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1372\u001b[0m         \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmatmul\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#buffer = ReplayBuffer(10000)\n",
    "buffer = PrioritizedReplayBuffer(20000)\n",
    "episode_reward = 0\n",
    "state = env.reset()\n",
    "state = torch.Tensor(state).cuda()\n",
    "state = state.unsqueeze(0)\n",
    "count = 0\n",
    "loss_count = 0\n",
    "rewards_dict = {}\n",
    "loss_dict = {}\n",
    "losses = []\n",
    "rewards = [] \n",
    "for i in tqdm(range(1,ITERATIONS+1)):\n",
    "    epsilon = epsilon_decay(i)\n",
    "    action = eps_greedy(epsilon,state,net)\n",
    "    next_state, reward, done, info = env.step(action)\n",
    "    next_state = torch.Tensor(next_state).unsqueeze(0).cuda()\n",
    "#     print('state input in buffer ',state.shape)\n",
    "#     print('state input in buffer ',next_state.shape)\n",
    "    episode_reward+=reward\n",
    "#     reward = max(min(reward,1),-1)\n",
    "    buffer.add(state,action,reward,next_state,done)\n",
    "    state = next_state\n",
    "    if(buffer.getSize()>REPLAY_SAMPLE and i%4==0):\n",
    "        loss = compute_loss(BATCH_SIZE)\n",
    "#         losses.append(loss)\n",
    "#         if(i%100==0):\n",
    "#             print(\"Loss\",loss.item())\n",
    "#             addloss(i,loss,lossfile)\n",
    "        if i % loss_log_interval == 0:\n",
    "            loss_dict[i] = {}\n",
    "            loss_dict[i]['loss'] = loss.item()\n",
    "            loss_count += 1\n",
    "            if loss_count == 10:\n",
    "                loss_df = pd.DataFrame.from_dict(data = loss_dict, orient = 'index').reset_index()\n",
    "                if not os.path.exists(lossfile):\n",
    "                    loss_df.to_csv(lossfile,index=None, header='column_names')\n",
    "                else: # else it exists so append without writing the header\n",
    "                    loss_df.to_csv(lossfile, mode='a',index=None, header=False)\n",
    "                loss_dict = {}\n",
    "                loss_count = 0\n",
    "    if done:\n",
    "        count += 1\n",
    "        rewards.append(episode_reward)\n",
    "        if count%10 == 0:\n",
    "                print(i,count,episode_reward) \n",
    "        if count%episode_reward_interval == 0: \n",
    "                rewards_dict[i] = {}\n",
    "                rewards_dict[i]['episode'] = count\n",
    "                rewards_dict[i]['reward'] = episode_reward\n",
    "                rewards_df = pd.DataFrame.from_dict(data = rewards_dict, orient = 'index').reset_index()\n",
    "                if not os.path.exists(rewardsfile):\n",
    "                    rewards_df.to_csv(rewardsfile,index=None, header='column_names')\n",
    "                else: # else it exists so append without writing the header\n",
    "                    rewards_df.to_csv(rewardsfile, mode='a',index=None, header=False)\n",
    "                rewards_dict = {}\n",
    "        episode_reward = 0 \n",
    "        state = env.reset()\n",
    "        state = torch.Tensor(state).cuda()\n",
    "        state = state.unsqueeze(0)\n",
    "    if(i%T_upd==0):\n",
    "        update_target()\n",
    "\n",
    "    if(i%save_model_interval == 0):\n",
    "        torch.save(net.state_dict(),'./prioritized-logs/prioritized-model' + str(i) + '.pth')\n",
    "        torch.save(target_net.state_dict(),'./prioritized-logs/prioritized-model-target' + str(i) + '.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
